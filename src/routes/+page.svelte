<textarea name="" id="" cols="70" rows="20" bind:value={source_text}></textarea>
<button onclick={send_question}>send_question</button>
<h3>Answer</h3>
{answer?.text}
<script>

let source_text = `You are looking for studies that would help reveal talents, allow you to touch the culture nurtured for almost half a millennium, and want to get to know a state with an exceptionally high standard of living and its people? You will find all of this by choosing to study in the Netherlands. Here, there are more than 122,000 students from other countries, so you certainly won't feel alone. And that's not surprising - there is no other country in Europe that offers so many study programs in English. So, studying here, you won't have communication difficulties - many people here are proficient in English. Why choose studies in the Netherlands?
In the Netherlands, studies are partially funded; European Union citizens pay only €2530 per year, covering only part of the tuition. The majority of the program's cost is covered by the country's government (for non-EU citizens, studies cost over €8000).`

const question = "What is the price of education for Lithuanian students in the Netherlands?"

let answer = null

import { Document } from "langchain/document"
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter"
import { MemoryVectorStore } from "langchain/vectorstores/memory";
import { HuggingFaceTransformersEmbeddings } from "langchain/embeddings/hf_transformers"
import { Ollama } from "langchain/llms/ollama"
import { RetrievalQAChain } from "langchain/chains"

async function set_document_text(source_text) {
    const docs = [new Document({ pageContent: source_text })]
    const splitter = new RecursiveCharacterTextSplitter({
        chunkOverlap: 0, chunkSize: 500
    })
    const splitDocuments = await splitter.splitDocuments(docs)
    
    const vectorStore = await MemoryVectorStore.fromDocuments(
        splitDocuments,
        new HuggingFaceTransformersEmbeddings()
    );

    const model = new Ollama({
        baseUrl: "http://localhost:11434",
        model: "openchat"
    })
    return RetrievalQAChain.fromLLM(model, vectorStore.asRetriever())
}

async function send_question() {
    let chain = await set_document_text(source_text)
    answer = await chain.call({query: question})
}


</script>